---
title: "chapter4"
author: "Jukke Kaaronen"
date: "16/11/2020"
output: html_document
---

# Chapter 4 Clustering and Classification

## Library

```{r}
library(MASS)
library(tidyverse)
library(corrplot)
library(GGally)
```

## 1 The Boston Dataset

```{r}
data(Boston)
dim(Boston)
str(Boston)
```
  
  The Boston dataset contains 506 observations of 14 variables that contain residential, social, and environmental information of 506 towns in the Boston area. These include e.g median value of owner-occupied homes (medv), weighted mean of distances to five Boston employment centers (dis), per capita crime rate (crim) and nitrogen oxides concentration (nox). A full deescription for metadata can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).
  The Boston dataset for R is derived from two studies:  
  
  **Source:**  
  
  Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.

  Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.  
  
## 1.1 Graphical Overview  

A graphical overview of the Boston matrix can be produced with the `pairs` function. It is not the clearest read but some interpetations can be seen such as the relation between crime rate (crim) having a relation with median value (medv) in the top-right corner.
  
```{r}
pairs(Boston)
```
  
## 1.2 Summary of the Variables
  
  The `summary` function reveals there is a lot of variance in many of the variables:  
  
```{r}
summary(Boston)
```
  For example, the crime rate per capita differs between 0.006 and 88.976. The zoning (zn) indicates the proportion of residential land zoned for lots over 25,000 sq.ft. A difference between 0-100 means that some areas/towns are zoned to be more residential.  
  The chas variable is described as a "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)" in the metadata. Here it is deviant because it operates on a binary 1/0 output. I omit it from the dataset.
  
```{r}
b1 <- Boston[-4]
```

## 1.3 Correlation Matrix Plot

  A correlation matrix plot is an easier to read plot. Here, the dark blue circles indicate strong positive correlation and dark red circles indicate strong negative correlations. For example, median value of owner-occupied homes (medv) correlates negatively with the percent of lower status (socio-economic status) population (lstat). 
  
```{r}
cor_matrix<-cor(b1) %>% round(digits = 2)

corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

```

## 2 Creating a Dataframe

### 2.1 Scaling Data
  
  To create a standardised dataframe the values need to be scaled. This is done by subtracting columns means from the columns and dividing the difference with standard deviation. In R it can be done with the `scale()` function. *(Note to self, only works on numerical values)*
  The `class` function can check the class of the new data. Here the class is a matrix, which I change to a dataframe.
  
```{r}
b1_scaled <- scale(b1)

class(b1_scaled)

b1_scaled <- as.data.frame(b1_scaled)
```

  **A summary of the new dataframe**:

```{r}
summary(b1_scaled)
```
  The summary shows that the variables have now been scaled so that the mean is 0 for each variable.  
  
### 2.2 Crime Rate as a Categorical Variable
  
  For the purposes of the assignment I create a new four category variable of the crime rate (crim). The observations are assigned in categories based on quantile break points.  
  A quantile vector can be created with the `quantile` function that produces the 0%,25%,50%, 75%, and 100% of a variable. A character vector is created that contains the names that are desired for the four categories.  
  A new factor with 4 levels is created with the `cut()` function. The `breaks =` argument specifies the breaking points and `label =` argument the desired names for the categories. I create a new dataframe from omitting the old crime rate variable and replacing it with the new 4-category variable.

```{r}
qvec <- quantile(b1_scaled$crim)
nvec <- c("low", "low_med", "med_high", "high")

crime <- cut(b1_scaled$crim, breaks = qvec, include.lowest = TRUE, label = nvec)

b2 <- b1_scaled %>% dplyr::select(-crim)
b2 <- data.frame(b2, crime)

```
  
### 2.3 Creating Train and Test sets

In this section the dataframe created in the past sections is divided randomly into training (80% of the dataframe) and testing (20% of the dataframe) datasets. This can be done with the `sample()` function that is specified with the argument `size =`. For the size argument I create the value **n** which is the number of observations (rows) * 0.8 in the b2 dataframe. Train and test sets are created by selecting the rows that are saved in `ind` (train set), and then subtracting the rows in `ind` for the test set. The train set now has 404 observations and the test set 102 observations.

```{r}
n <- nrow(b2)

ind <- sample(n,  size = n * 0.8)

train <- b2[ind,]

test <- b2[-ind,]

dim(train)
dim(test)


```
  Next a new value needs to be created for the correct crime classes in the test dataframe, because these will be removed from the dataframe.

```{r}
correct_classes <- test$crime


test <- dplyr::select(test, -crime)
```

## 3 Linear Discriminant Analysis
  
### 3.1 Creating an LDA Model
  
Linear Discriminant Analysis is a classification method that can be used to find variables that discriminate categorical classes, and predict the classes of new data. In this section LDA is performed on the categorical crime rate variable created in the previous sections.  
An LDA formula is created with the `lda()` function. For the purposes of this task it will be predicted by all the other variables in the train dataframe.


```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 3)
```
  LD1 axis accounts for the most variation between the categories. Here, especially the **rad** (index of accessibility to radial highways) variable stands out as a strong coefficient for the high crime rate category, which is fairly distinctly separated in the plot on the LD1 axis.  
The LD2 axis accounts for the second most variation between the categories. Here, the **zn** (proportion of residential land zoned for lots over 25,000 sq.ft.) and **nox** (nitrogen oxides concentration) along with the **rad** are the strongest coefficients. The LD2 axis in the biplot shows some separation between the low, low_mid and mid_high categories though this separation is overlapping.

## 3.2 Performing Prediction

The LDA model created in the previous section from the training dataset can be used to predict the testing dataset. This is done with the `predict()` function. The `newdata =` argument specifies the dataset to predict on. The test dataframe, in this case.  

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```
  
  A cross tabulation of the correct categories and the predicted categories shows that the LDA model was good at predicting high crime rate, making only 1 error. There were 17 correct predictions of med_high and 17 incorrect ones. 17 correct predictions of low_med and 10 incorrect. 12 correct predictions of low and 10 incorrect. Low crime rates were never predicted as high and vice versa, which is good.

## 4 K-Means Clustering

### 4.1 Calculating Distance
  
  To calculate distance use the `dist()` function. A summary shows the mean distance for all variables to be 4.91, but with quite a bit of variation.

```{r}
data("Boston")
b3 <- scale(Boston)
b3 <- as.data.frame(b3)
summary(b3)

euclidean <- dist(b3)
summary(euclidean)
```
### 4.2 Running the k-means algorithm

  To perform the `kmeans()` function it is good to know the number of optimal clusters first. This can be done by plotting the total within cluster sum of squares with the code below. The `qplot()` shows where the within cluster sum of squares falls radically and this indicates the optimal number of clusters. In this case the optimal number of clusters is small, with only 2 optimal clusters. The optimal number of clusters is attributed in the `centers =` argument of the `kmeans()` function.

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(b3, k)$tot.withinss})

qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(b3, centers = 2)

pairs(b3, col = km$cluster)
```

   This ~~clusterfuck~~ plot shows the 2 centered clusters in each variable pairing. In the previous section, I discussed how the accessibility index for radial highways (**rad**) was a strong coefficient in the LDA model. Here too, we see the 2 clusters from the k-means algorithm fairly well distinguished in the **rad** pairings. The same can be said, to a lesser extent, about the **tax**, **zn** and **nox**, for example. 